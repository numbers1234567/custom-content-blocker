{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Reddit Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ[\"CONTENT_CURATION_REDDIT_API_CLIENT_ID\"],\n",
    "    client_secret=os.environ[\"CONTENT_CURATION_REDDIT_API_CLIENT_SECRET\"],\n",
    "    password=os.environ[\"CONTENT_CURATION_REDDIT_API_PASSWORD\"],\n",
    "    user_agent=os.environ[\"CONTENT_CURATION_REDDIT_API_USER_AGENT\"],\n",
    "    username=os.environ[\"CONTENT_CURATION_REDDIT_API_USERNAME\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_html_embed(permalink : str):\n",
    "    reformatted_url = f\"https://www.reddit.com{permalink}\"\n",
    "    \n",
    "    reformatted_url.replace(\":\", \"%3A\").replace(\"/\", \"%2F\")\n",
    "    return json.loads(requests.get(f'https://www.reddit.com/oembed?url={reformatted_url}').content)[\"html\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular = reddit.subreddit(\"popular\").hot(limit=100)\n",
    "popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data = [(idx, \n",
    "          'https://www.reddit.com'+post.permalink, \n",
    "          post.title, \n",
    "          get_html_embed(post.permalink).replace(\"\\n\", \"\"), \n",
    "          int(post.created_utc),\n",
    "          post) for idx,post in enumerate(popular)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get BLIP Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../postgres-db-manager\")\n",
    "import base64\n",
    "from SocialAPIHandlers.RedditClient import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_image_url(url : str) -> str:\n",
    "    return url.replace(\"&amp;\", \"&\")\n",
    "def get_imgs_b64(post) -> list[str]:\n",
    "\n",
    "    base_url = post.url\n",
    "    img_urls = []\n",
    "    # It is a single image\n",
    "    if \"i.redd.it\" in base_url:\n",
    "        img_urls.append(base_url)\n",
    "\n",
    "    # It is a gallery of images\n",
    "    try:  # Possible things could go wrong with all of these accesses\n",
    "        for media in post.gallery_data['items']:\n",
    "            media_id = media[\"media_id\"]\n",
    "\n",
    "            metadata = post.media_metadata[media_id]\n",
    "\n",
    "            # Only images\n",
    "            if metadata[\"e\"] != \"image\":\n",
    "                continue\n",
    "\n",
    "            best_version = (-1000, \"\")  # (size, url)\n",
    "            for version in metadata['p'] + [metadata['s']]:\n",
    "                size = version[\"x\"]*version[\"y\"]\n",
    "                if size > 1000*1000: continue  # Too big \n",
    "\n",
    "                best_version = max(best_version, (size, version['u']))\n",
    "            if best_version[0] > 0:\n",
    "                img_urls.append(clean_image_url(best_version[1]))\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # Convert images\n",
    "    imgs_b64 = []\n",
    "    for url in img_urls:\n",
    "        try:\n",
    "            imgs_b64.append(base64.encodebytes(requests.get(url).content))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return imgs_b64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import get_inference_endpoint\n",
    "import os\n",
    "# HuggingFace inference endpoints\n",
    "_HUGGINGFACE_ENDPOINT_NAME = os.environ[\"CONTENT_CURATION_HUGGINGFACE_ENDPOINT_NAME\"]\n",
    "_HUGGINGFACE_ACCESS_TOKEN  = os.environ[\"CONTENT_CURATION_HUGGINGFACE_ACCESS_TOKEN\"]\n",
    "\n",
    "huggingface_blip_endpoint = get_inference_endpoint(name=_HUGGINGFACE_ENDPOINT_NAME, token=_HUGGINGFACE_ACCESS_TOKEN)\n",
    "def get_blip_features(text:str, has_image:bool, base_64_image:str|None=None):\n",
    "    body = {\n",
    "        \"inputs\" : {\n",
    "            \"text\" : text, \n",
    "            \"has_image\" : base_64_image!=None, \n",
    "            \"image\" : base_64_image \n",
    "        }\n",
    "    }\n",
    "\n",
    "    feature_vector = json.loads(huggingface_blip_endpoint.client.post(json=body))[\"feature_vector\"][0]\n",
    "\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for post in post_data:\n",
    "    try:\n",
    "        images = get_imgs_b64(post[5])\n",
    "        feature_vector = get_blip_features(post[2], len(images) > 0, images[0].decode('utf-8') if len(images) > 0 else None)\n",
    "        features.append((post[0], feature_vector))\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with open(\"test_post_data.csv\", \"w+\") as f:\n",
    "    f.write(\"internal_id\\tpost_id\\ttext\\tembed_html\\tcreate_utc\\n\")\n",
    "    for idx,post in enumerate(post_data):\n",
    "        f.write(str(post[0]) + \"\\t\")\n",
    "        f.write(str(post[1]) + \"\\t\")\n",
    "        f.write(str(post[2]) + \"\\t\")\n",
    "        f.write(str(post[3]) + \"\\t\")\n",
    "        f.write(str(post[4]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_blip_data.csv\", \"w+\") as f:\n",
    "    f.write(\"internal_id\\tfeatures\\n\")\n",
    "    for idx,feature_vector in features:\n",
    "        f.write(str(idx) + \"\\t\")\n",
    "        f.write(\"{\" + \",\".join([str(i) for i in feature_vector]) + \"}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\",\".join([str(i) for i in features[0][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake credentials and creation times\n",
    "random.seed(42)\n",
    "whitelist_characters = \"qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM\"\n",
    "with open(\"test_user_data.csv\", \"w+\") as f:\n",
    "    f.write(\"user_id\\tcreate_utc\\temail\\n\")\n",
    "    for i in range(100):\n",
    "        create_time = int(1723760179 + random.randint(-200, 200))\n",
    "        if i == 0: email = \"contentcuratorauth@gmail.com\"\n",
    "        else: email = \"\".join([random.choice(whitelist_characters) for _ in range(20)]) + \"@gmail.com\"\n",
    "        f.write(str(i) + \"\\t\")\n",
    "        f.write(str(create_time) + \"\\t\")\n",
    "        f.write(str(email) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "curation_ids = list(range(2,302))\n",
    "random.shuffle(curation_ids)\n",
    "whitelist_characters = \"qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM\"\n",
    "with open(\"test_curation_data.csv\", \"w+\") as f:\n",
    "    f.write(\"primary_user\\tcuration_id\\tcuration_name\\tcuration_key\\tcreate_utc\\n\")\n",
    "    for i in range(300):\n",
    "        primary_user = random.randint(0,99)\n",
    "        curation_id = curation_ids[i]\n",
    "        curation_name = \"\".join([random.choice(whitelist_characters) for _ in range(20)])\n",
    "        curation_key = \"\".join([random.choice(whitelist_characters) for _ in range(40)])\n",
    "        create_utc = int(1723760179 + random.randint(-200, 200))\n",
    "        f.write(str(primary_user) + \"\\t\")\n",
    "        f.write(str(curation_id) + \"\\t\")\n",
    "        f.write(str(curation_name) + \"\\t\")\n",
    "        f.write(str(curation_key) + \"\\t\")\n",
    "        f.write(str(create_utc) + \"\\n\")\n",
    "    f.write(\"0\\t0\\tNo Politics\\tno_politics\\t0\\n\")\n",
    "    f.write(\"0\\t1\\tPolitics Only\\tpolitics_only\\t0\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create randomized BLIP heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"test_curation_data.csv\", delimiter=\"\\t\")\n",
    "\n",
    "curate_ids = list(df[\"curation_id\"])\n",
    "primary_users = list(df[\"primary_user\"])\n",
    "data = zip(curate_ids, primary_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure uniqueness\n",
    "len(set(curate_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "def create_formatted_str_array(arr : Iterable[any]) -> str:\n",
    "    return \"{\" + \",\".join([str(i) for i in arr]) + \"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIPHead(nn.Module):\n",
    "    # BlipDeepHead4\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        seq = [\n",
    "            nn.Linear(768, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(10, 2),\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "        self.mlp = nn.Sequential(\n",
    "            *seq\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.mlp(features)\n",
    "    \n",
    "no_politics_head = BLIPHead()\n",
    "no_politics_head.load_state_dict(torch.load(\"test_no_politics_head\", map_location=torch.device(\"cpu\")))\n",
    "no_politics_head.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "with open(\"test_blip_heads.csv\", \"w+\") as f:\n",
    "    f.write(\"curation_id\\tweight1\\tweight2\\tbias1\\tbias2\\n\")\n",
    "    for curate_id,user in data:\n",
    "        f.write(str(curate_id)+\"\\t\")\n",
    "        # https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073\n",
    "        stdv1 = 1. / np.sqrt(768)\n",
    "        stdv2 = 1. / np.sqrt(10)\n",
    "        if curate_id==0:  # No_politics\n",
    "            w1 = np.transpose(no_politics_head.mlp[0].weight.detach().numpy(), (1,0))\n",
    "            b1 = no_politics_head.mlp[0].bias.detach().numpy()\n",
    "            w2 = np.transpose(no_politics_head.mlp[3].weight.detach().numpy(), (1,0))\n",
    "            b2 = no_politics_head.mlp[3].bias.detach().numpy()\n",
    "        elif curate_id==1: # politics_only\n",
    "            w1 = np.transpose(no_politics_head.mlp[0].weight.detach().numpy(), (1,0))\n",
    "            b1 = no_politics_head.mlp[0].bias.detach().numpy()\n",
    "            w2 = np.transpose(no_politics_head.mlp[3].weight.detach().numpy()[::-1], (1,0))\n",
    "            b2 = no_politics_head.mlp[3].bias.detach().numpy()[::-1]\n",
    "        else:\n",
    "            w1,b1 = np.random.rand(768, 10)*(2*stdv1) - stdv1, np.random.rand(10)*(2*stdv1) - stdv1\n",
    "            w2,b2 = np.random.rand(10, 2)*(2*stdv2) - stdv2, np.random.rand(2)*(2*stdv2) - stdv2\n",
    "\n",
    "        f.write(create_formatted_str_array([create_formatted_str_array(row) for row in w1])+\"\\t\")\n",
    "        f.write(create_formatted_str_array([create_formatted_str_array(row) for row in w2])+\"\\t\")\n",
    "        f.write(create_formatted_str_array(b1)+\"\\t\")\n",
    "        f.write(create_formatted_str_array(b2)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Emerging Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Use my production database to get data\n",
    "\n",
    "_POSTGRES_DB_NAME = os.environ[\"CONTENT_CURATION_POSTGRES_DB_NAME\"]\n",
    "_POSTGRES_DB_USER = os.environ[\"CONTENT_CURATION_POSTGRES_USER\"]\n",
    "_POSTGRES_DB_PASS = os.environ[\"CONTENT_CURATION_POSTGRES_PASSWORD\"]\n",
    "_POSTGRES_DB_HOST = os.environ[\"CONTENT_CURATION_POSTGRES_HOST\"]\n",
    "_POSTGRES_DB_PORT = os.environ[\"CONTENT_CURATION_POSTGRES_PORT\"]\n",
    "\n",
    "POSTGRES_DB_URL = f'postgres://{_POSTGRES_DB_USER}:{_POSTGRES_DB_PASS}@{_POSTGRES_DB_HOST}:{_POSTGRES_DB_PORT}/{_POSTGRES_DB_NAME}'\n",
    "\n",
    "with psycopg2.connect(POSTGRES_DB_URL) as conn, \\\n",
    "    open(\"test_emerging_topics.csv\", \"w+\") as f1, \\\n",
    "    open(\"test_emerging_topic_ngram.csv\", \"w+\") as f2:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT topic_id,topic_name,topic_key,create_utc,date_start,date_end\n",
    "        FROM emerging_topic\n",
    "        WHERE topic_id < 100;\n",
    "    \"\"\")\n",
    "    emerging_topics = cur.fetchall()\n",
    "    f1.write(\"topic_id\\ttopic_name\\ttopic_key\\tcreate_utc\\tdate_start\\tdate_end\\n\")\n",
    "    for topic_id,topic_name,topic_key,create_utc,date_start,date_end in emerging_topics:\n",
    "        f1.write(f\"{topic_id}\\t{topic_name}\\t{topic_key}\\t{create_utc}\\t{date_start}\\t{date_end}\\n\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT topic_id,ngram\n",
    "        FROM emerging_topic_ngram\n",
    "        WHERE topic_id < 100;\n",
    "    \"\"\")\n",
    "    emerging_topics_ngram = cur.fetchall()\n",
    "    f2.write(\"topic_id\\tngram\\n\")\n",
    "    for topic_id,ngram in emerging_topics_ngram:\n",
    "        f2.write(f\"{topic_id}\\t{ngram}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social-media-db-manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

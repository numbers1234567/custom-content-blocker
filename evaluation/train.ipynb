{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIP NLVR\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPNLVRHead(med_config=\"BLIP/configs/med_config.json\", device=device), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_nlvr/blip_nlvr_mlp_e{i}\"\n",
    "save_path(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIP w/ CLS Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPDeepHead1(med_config=\"BLIP/configs/med_config.json\", device=device), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_deep_mlp_1/blip_deep_mlp_1_e{i}\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIP w/ CLS Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPDeepHead2(med_config=\"BLIP/configs/med_config.json\", device=device), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_deep_mlp_2/blip_deep_mlp_2_e{i}\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIP w/ CLS Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPDeepHead3(med_config=\"BLIP/configs/med_config.json\", device=device), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_deep_mlp_3/blip_deep_mlp_3_e{i}\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIP w/ SVM Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPSVMHead(), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_svm/blip_svm\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RedditData import *\n",
    "\n",
    "train_dataset = RedditDataset(\"../datasets\", main_csv=\"../datasets/main_val.csv\", im_transform=train_transform)\n",
    "train_sampler = RedditDataSampler(\"../datasets\", main_csv=\"../datasets/main_val.csv\")\n",
    "\n",
    "val_dataset = RedditDataset(\"../datasets\", main_csv=\"../datasets/main_val.csv\", split=\"valid\", im_transform=val_transform)\n",
    "val_sampler = RedditDataSampler(\"../datasets\", main_csv=\"../datasets/main_val.csv\", split=\"valid\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_sampler=BatchSampler(train_sampler, batch_size, True))\n",
    "val_dataloader = DataLoader(val_dataset, batch_sampler=BatchSampler(val_sampler, 1, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "def validation(classifier, val_dataloader):\n",
    "    loss = CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        loss_val = 0\n",
    "        n = len(val_dataloader)\n",
    "        for i in val_dataloader:\n",
    "            text,image,label,has_image = i\n",
    "            image = image.float()\n",
    "            label = F.one_hot(label, num_classes=2).float()\n",
    "            loss_val += loss(torch.from_numpy(classifier(image, text, has_image)).float(), label).item()/n\n",
    "\n",
    "        return loss_val\n",
    "    \n",
    "#validation(classifier, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(10):\n",
    "    pbar = tqdm(train_dataloader, \"Loss - \")\n",
    "    for idx,data in enumerate(pbar):\n",
    "        text,image,label,has_image = data\n",
    "        image = image.float()\n",
    "        label = F.one_hot(label, num_classes=2).float()\n",
    "        classifier.train(image,text,label,has_image)\n",
    "        if idx==len(pbar)-1:\n",
    "            pbar.set_description(\"Validation loss - %f\" % (validation(classifier, val_dataloader)))\n",
    "    classifier.save(save_path(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    classifier.load(save_path(i))\n",
    "    print(validation(classifier, val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_output(classifier, dataloader):\n",
    "    ret = []\n",
    "    for batch in dataloader:\n",
    "        text,image,label,has_image = batch\n",
    "        image = image.float()\n",
    "        yp = classifier(image,text,has_image)\n",
    "        for i in range(len(text)):\n",
    "            ret.append(((text[i],image[i].cpu().detach().numpy(),label[i].cpu().detach().numpy(),has_image[i].cpu().detach().numpy()),yp[i]))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "def get_failure_cases(model_out, thresh=0.5):\n",
    "    idx = []\n",
    "    for i in range(len(model_out)):\n",
    "        (text,image,label,has_image),pred = model_out[i]\n",
    "        gt = label\n",
    "        ans = 1 if pred[1] >= thresh else 0\n",
    "        if ans != gt: idx.append(i)\n",
    "    return idx\n",
    "\n",
    "def get_metrics(model_out):\n",
    "    scores = [i[1][1] for i in model_out]\n",
    "    roc=roc_curve([i[0][2] for i in model_out], scores)\n",
    "    auc = roc_auc_score([i[0][2] for i in model_out], scores)\n",
    "    num1 = sum([i[0][2] for i in model_out])\n",
    "    num0 = len(model_out) - num1\n",
    "    fp = sum([1 if i[0][2]==0 and np.round(i[1][1])==1 else 0 for i in model_out])\n",
    "    fn = sum([1 if i[0][2]==1 and np.round(i[1][1])==0 else 0 for i in model_out])\n",
    "    tp = sum([1 if i[0][2]==1 and np.round(i[1][1])==1 else 0 for i in model_out])\n",
    "    tn = sum([1 if i[0][2]==0 and np.round(i[1][1])==0 else 0 for i in model_out])\n",
    "    correct = sum([1 if i[0][2]==np.round(i[1][1]) else 0 for i in model_out])\n",
    "    return roc,auc,fp,fn,tp,tn,num0,num1,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load(save_path(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = get_val_output(classifier, val_dataloader)\n",
    "roc,auc,fp,fn,tp,tn,num0,num1,correct = get_metrics(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresh = roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc,fp,fn,tp,tn,num0,num1,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.plot(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fails = get_failure_cases(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([model_out[fail][0][2] for fail in fails])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=14\n",
    "(text,image,label,has_image),yp = model_out[i]\n",
    "text,label,has_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "im = np.transpose(image, (1,2,0))\n",
    "\n",
    "plt.imshow(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_content_blocker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from PIL import image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Sampler, BatchSampler\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "class RedditDataset(Dataset):\n",
    "    def __init__(self, root_dir, main_csv=\"main.csv\", media_csv=\"media.csv\", split=\"train\", im_transform=transforms.Compose([transforms.ToTensor(), transforms.Resize((224,224),interpolation=InterpolationMode.BICUBIC)]), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.main_csv  = os.path.join(root_dir, main_csv)\n",
    "        self.media_csv = os.path.join(root_dir, media_csv)\n",
    "        self.root_dir  = root_dir\n",
    "        self.split = \"train\"\n",
    "\n",
    "        self.main_df   = pd.read_csv(self.main_csv, delimiter=\"\\t\")\n",
    "        self.main_df   = self.main_df[self.main_df[\"split\"]==split]\n",
    "        self.media_df  = pd.read_csv(self.media_csv, delimiter=\"\\t\")\n",
    "\n",
    "        self.transform = im_transform\n",
    "\n",
    "    def __len__(self): return len(self.main_df)\n",
    "    def __getitem__(self, index):\n",
    "        # A little inefficient, but data is going to be small.\n",
    "        main_row = self.main_df.loc[self.main_df[\"ID\"]==index]\n",
    "        text = main_row[\"text\"].iloc[0]\n",
    "        label = main_row[\"label\"].iloc[0]\n",
    "\n",
    "        image_paths = self.media_df.loc[self.media_df[\"ID\"]==index][\"path\"]\n",
    "        image_paths = [os.path.join(self.root_dir, path) for path in image_paths]\n",
    "        \n",
    "        images = [Image.open(path).convert('RGB') for path in image_paths][1::2]\n",
    "        images = [im for im in images if im is not None]\n",
    "\n",
    "        image = Image.new('RGB', (1000, 1000))\n",
    "        if len(images) > 0:\n",
    "            image = random.choice(images)\n",
    "        #image = image.transpose((2,0,1))\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return text,image,label\n",
    "    \n",
    "class RedditDataSampler(Sampler):\n",
    "    def __init__(self, root_dir, main_csv=\"main.csv\", split=\"train\"):\n",
    "        super().__init__()\n",
    "        main_csv  = os.path.join(root_dir, main_csv)\n",
    "        main_df = pd.read_csv(main_csv, delimiter=\"\\t\")\n",
    "        self.indices = list(main_df[main_df[\"split\"]==split][\"ID\"])\n",
    "        random.shuffle(self.indices)\n",
    "        self.cur_index = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            if self.cur_index==len(self.indices): # Seen all data\n",
    "                random.shuffle(self.indices)\n",
    "                self.cur_index = 0\n",
    "            ret = self.indices[self.cur_index]\n",
    "            self.cur_index += 1\n",
    "            yield ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_img_size = 224\n",
    "\n",
    "blip_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])  \n",
    "\n",
    "dataset = RedditDataset(\"../datasets\", im_transform=blip_transform)\n",
    "#dataset = RedditDataset(\"../datasets\")\n",
    "sampler = RedditDataSampler(\"../datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(BatchSampler(sampler, batch_size=2, drop_last=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_sampler=BatchSampler(sampler, 2, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][1][:,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(np.transpose(dataset[0][1].cpu().detach().numpy(), (1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_demo_image(image_size,device):\n",
    "    img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')   \n",
    "\n",
    "    w,h = raw_image.size\n",
    "    display(raw_image.resize((w//5,h//5)))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ]) \n",
    "    image = transform(raw_image).unsqueeze(0).to(device)   \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../model/BLIP\")\n",
    "sys.path.append(\"../model\")\n",
    "from models.blip import *\n",
    "\n",
    "image_size = 224\n",
    "image = load_demo_image(image_size=image_size, device=device)     \n",
    "\n",
    "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth'\n",
    "    \n",
    "model = blip_feature_extractor(pretrained=model_url, image_size=image_size, vit='base', med_config=\"../model/BLIP/configs/med_config.json\")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "caption = 'a woman sitting on the beach with a dog'\n",
    "\n",
    "multimodal_feature = model(image, caption, mode='multimodal')[0,0]\n",
    "image_feature = model(image, caption, mode='image')[0,0]\n",
    "text_feature = model(image, caption, mode='text')[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIP NLVR\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPNLVRHead(med_config=\"BLIP/configs/med_config.json\", device=device), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_nlvr/blip_nlvr_mlp_e{i}\"\n",
    "save_path(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIP w/ CLS Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPDeepHead1(med_config=\"BLIP/configs/med_config.json\", device=device), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_deep_mlp_1/blip_deep_mlp_1_e{i}\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIP w/ CLS Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPDeepHead2(med_config=\"BLIP/configs/med_config.json\", device=device), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_deep_mlp_2/blip_deep_mlp_2_e{i}\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIP w/ CLS Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPDeepHead3(med_config=\"BLIP/configs/med_config.json\", device=device), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_deep_mlp_3/blip_deep_mlp_3_e{i}\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIP w/ SVM Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPSVMHead(), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_svm/blip_svm\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RedditData import *\n",
    "\n",
    "train_dataset = RedditDataset(\"../datasets\", main_csv=\"../datasets/main_val.csv\", im_transform=train_transform)\n",
    "train_sampler = RedditDataSampler(\"../datasets\", main_csv=\"../datasets/main_val.csv\")\n",
    "\n",
    "val_dataset = RedditDataset(\"../datasets\", main_csv=\"../datasets/main_val.csv\", split=\"valid\", im_transform=val_transform)\n",
    "val_sampler = RedditDataSampler(\"../datasets\", main_csv=\"../datasets/main_val.csv\", split=\"valid\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_sampler=BatchSampler(train_sampler, batch_size, True))\n",
    "val_dataloader = DataLoader(val_dataset, batch_sampler=BatchSampler(val_sampler, 1, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "def validation(classifier, val_dataloader):\n",
    "    loss = CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        loss_val = 0\n",
    "        n = len(val_dataloader)\n",
    "        for i in val_dataloader:\n",
    "            text,image,label,has_image = i\n",
    "            image = image.float()\n",
    "            label = F.one_hot(label, num_classes=2).float()\n",
    "            loss_val += loss(torch.from_numpy(classifier(image, text, has_image)).float(), label).item()/n\n",
    "\n",
    "        return loss_val\n",
    "    \n",
    "#validation(classifier, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(10):\n",
    "    pbar = tqdm(train_dataloader, \"Loss - \")\n",
    "    for idx,data in enumerate(pbar):\n",
    "        text,image,label,has_image = data\n",
    "        image = image.float()\n",
    "        label = F.one_hot(label, num_classes=2).float()\n",
    "        classifier.train(image,text,label,has_image)\n",
    "        if idx==len(pbar)-1:\n",
    "            pbar.set_description(\"Validation loss - %f\" % (validation(classifier, val_dataloader)))\n",
    "    classifier.save(save_path(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    classifier.load(save_path(i))\n",
    "    print(validation(classifier, val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_output(classifier, dataloader):\n",
    "    ret = []\n",
    "    for batch in dataloader:\n",
    "        text,image,label,has_image = batch\n",
    "        image = image.float()\n",
    "        yp = classifier(image,text,has_image)\n",
    "        for i in range(len(text)):\n",
    "            ret.append(((text[i],image[i].cpu().detach().numpy(),label[i].cpu().detach().numpy(),has_image[i].cpu().detach().numpy()),yp[i]))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "def get_failure_cases(model_out, thresh=0.5):\n",
    "    idx = []\n",
    "    for i in range(len(model_out)):\n",
    "        (text,image,label,has_image),pred = model_out[i]\n",
    "        gt = label\n",
    "        ans = 1 if pred[1] >= thresh else 0\n",
    "        if ans != gt: idx.append(i)\n",
    "    return idx\n",
    "\n",
    "def get_metrics(model_out):\n",
    "    scores = [i[1][1] for i in model_out]\n",
    "    roc=roc_curve([i[0][2] for i in model_out], scores)\n",
    "    auc = roc_auc_score([i[0][2] for i in model_out], scores)\n",
    "    num1 = sum([i[0][2] for i in model_out])\n",
    "    num0 = len(model_out) - num1\n",
    "    fp = sum([1 if i[0][2]==0 and np.round(i[1][1])==1 else 0 for i in model_out])\n",
    "    fn = sum([1 if i[0][2]==1 and np.round(i[1][1])==0 else 0 for i in model_out])\n",
    "    tp = sum([1 if i[0][2]==1 and np.round(i[1][1])==1 else 0 for i in model_out])\n",
    "    tn = sum([1 if i[0][2]==0 and np.round(i[1][1])==0 else 0 for i in model_out])\n",
    "    correct = sum([1 if i[0][2]==np.round(i[1][1]) else 0 for i in model_out])\n",
    "    return roc,auc,fp,fn,tp,tn,num0,num1,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load(save_path(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = get_val_output(classifier, val_dataloader)\n",
    "roc,auc,fp,fn,tp,tn,num0,num1,correct = get_metrics(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresh = roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc,fp,fn,tp,tn,num0,num1,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.plot(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fails = get_failure_cases(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([model_out[fail][0][2] for fail in fails])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=14\n",
    "(text,image,label,has_image),yp = model_out[i]\n",
    "text,label,has_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "im = np.transpose(image, (1,2,0))\n",
    "\n",
    "plt.imshow(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_content_blocker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

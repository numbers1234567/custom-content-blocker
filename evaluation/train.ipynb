{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from BLIP/blip_base.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_checkpoints/blip_nlvr/blip_nlvr_mlp_e5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLIP NLVR\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPNLVRHead(med_config=\"BLIP/configs/med_config.json\", device=device, dropout=True), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device, text_inject=True)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_nlvr/blip_nlvr_mlp_e{i}\"\n",
    "save_path(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from BLIP/blip_base.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_checkpoints/blip_deep_mlp_1/blip_deep_mlp_1_e5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLIP w/ CLS Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPDeepHead1(med_config=\"BLIP/configs/med_config.json\", device=device), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device, text_inject=True)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_deep_mlp_1/blip_deep_mlp_1_e{i}\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from BLIP/blip_base.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_checkpoints/blip_deep_mlp_2/blip_deep_mlp_2_e5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLIP w/ CLS Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPDeepHead2(med_config=\"BLIP/configs/med_config.json\", device=device), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device, text_inject=True)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_deep_mlp_2/blip_deep_mlp_2_e{i}\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from BLIP/blip_base.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_checkpoints/blip_deep_mlp_3/blip_deep_mlp_3_e5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLIP w/ CLS Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPDeepHead3(med_config=\"BLIP/configs/med_config.json\", device=device, dropout=True), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device, text_inject=True)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_deep_mlp_3/blip_deep_mlp_3_e{i}\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from BLIP/blip_base.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_checkpoints/blip_deep_mlp_4/blip_deep_mlp_4_e5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLIP w/ CLS Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPDeepHead4(med_config=\"BLIP/configs/med_config.json\", device=device, dropout=True), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device, text_inject=True)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_deep_mlp_4/blip_deep_mlp_4_e{i}\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from BLIP/blip_base.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_checkpoints/blip_deep_mlp_4/blip_deep_mlp_4_e5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLIP w/ CLS Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPDeepHead5(med_config=\"BLIP/configs/med_config.json\", device=device, dropout=True), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device, text_inject=True)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_deep_mlp_5/blip_deep_mlp_5_e{i}\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLIP w/ SVM Head\n",
    "import sys\n",
    "sys.path.append(\"BLIP\")\n",
    "from pipelines_train import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classifier = BLIPClassifier(BLIPSVMHead(), med_config=\"BLIP/configs/med_config.json\", pretrain_path=\"BLIP/blip_base.pth\", device=device)\n",
    "\n",
    "blip_img_size = 224\n",
    "batch_size = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((blip_img_size,blip_img_size),interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "val_transform = train_transform\n",
    "\n",
    "def save_path(i):\n",
    "    return f\"model_checkpoints/blip_svm/blip_svm\"\n",
    "save_path(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RedditData import *\n",
    "\n",
    "main_csv_path = \"../datasets/main_val.csv\"\n",
    "\n",
    "train_dataset = RedditDataset(\"../datasets\", main_csv=main_csv_path, im_transform=train_transform)\n",
    "train_sampler = RedditDataSampler(\"../datasets\", main_csv=main_csv_path, upsampling=False)\n",
    "\n",
    "val_dataset = RedditDataset(\"../datasets\", main_csv=main_csv_path, split=\"valid\", im_transform=val_transform)\n",
    "val_sampler = RedditDataSampler(\"../datasets\", main_csv=main_csv_path, split=\"valid\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_sampler=BatchSampler(train_sampler, batch_size, True))\n",
    "val_dataloader = DataLoader(val_dataset, batch_sampler=BatchSampler(val_sampler, 1, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "def validation(classifier, val_dataloader):\n",
    "    loss = CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        loss_val = 0\n",
    "        n = len(val_dataloader)\n",
    "        for i in val_dataloader:\n",
    "            text,image,label,has_image = i\n",
    "            image = image.float()\n",
    "            label = F.one_hot(label, num_classes=2).float()\n",
    "            loss_val += loss(torch.from_numpy(classifier(image, text, has_image)).float(), label).item()/n\n",
    "\n",
    "        return loss_val\n",
    "    \n",
    "#validation(classifier, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss - 0.591642: 100%|██████████| 103/103 [03:17<00:00,  1.91s/it]\n",
      "Validation loss - 0.534635: 100%|██████████| 103/103 [03:11<00:00,  1.86s/it]\n",
      "Validation loss - 0.530278: 100%|██████████| 103/103 [03:26<00:00,  2.00s/it]\n",
      "Validation loss - 0.470381: 100%|██████████| 103/103 [03:28<00:00,  2.02s/it]\n",
      "Validation loss - 0.456086: 100%|██████████| 103/103 [03:21<00:00,  1.95s/it]\n",
      "Validation loss - 0.455295: 100%|██████████| 103/103 [03:28<00:00,  2.02s/it]\n",
      "Validation loss - 0.463725: 100%|██████████| 103/103 [03:07<00:00,  1.82s/it]\n",
      "Validation loss - 0.452340: 100%|██████████| 103/103 [03:02<00:00,  1.78s/it]\n",
      "Validation loss - 0.453566: 100%|██████████| 103/103 [03:01<00:00,  1.77s/it]\n",
      "Validation loss - 0.448967: 100%|██████████| 103/103 [03:01<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(10):\n",
    "    pbar = tqdm(train_dataloader, \"Loss - \")\n",
    "    for idx,data in enumerate(pbar):\n",
    "        text,image,label,has_image = data\n",
    "        image = image.float()\n",
    "        label = F.one_hot(label, num_classes=2).float()\n",
    "        classifier.train(image,text,label,has_image)\n",
    "        if idx==len(pbar)-1:\n",
    "            pbar.set_description(\"Validation loss - %f\" % (validation(classifier, val_dataloader)))\n",
    "    classifier.save(save_path(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.565385476258642\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      2\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mload(save_path(i))\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mvalidation\u001b[1;34m(classifier, val_dataloader)\u001b[0m\n\u001b[0;32m      6\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_dataloader)\n\u001b[1;32m----> 8\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhas_image\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\angel\\anaconda3\\envs\\custom_content_blocker\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\angel\\anaconda3\\envs\\custom_content_blocker\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\angel\\anaconda3\\envs\\custom_content_blocker\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\angel\\anaconda3\\envs\\custom_content_blocker\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\angel\\Documents\\programming-projects\\content-curation\\custom-content-blocker\\evaluation\\RedditData.py:39\u001b[0m, in \u001b[0;36mRedditDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     36\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmedia_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmedia_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39mindex][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     37\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, path) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m image_paths]\n\u001b[1;32m---> 39\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     40\u001b[0m images \u001b[38;5;241m=\u001b[39m [im \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m images \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     42\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m1000\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\angel\\Documents\\programming-projects\\content-curation\\custom-content-blocker\\evaluation\\RedditData.py:39\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmedia_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmedia_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39mindex][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     37\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, path) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m image_paths]\n\u001b[1;32m---> 39\u001b[0m images \u001b[38;5;241m=\u001b[39m [\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m image_paths][\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     40\u001b[0m images \u001b[38;5;241m=\u001b[39m [im \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m images \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     42\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m1000\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\angel\\anaconda3\\envs\\custom_content_blocker\\Lib\\site-packages\\PIL\\Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    876\u001b[0m ):\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\angel\\anaconda3\\envs\\custom_content_blocker\\Lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    classifier.load(save_path(i))\n",
    "    print(validation(classifier, val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_output(classifier, dataloader):\n",
    "    ret = []\n",
    "    for batch in dataloader:\n",
    "        text,image,label,has_image = batch\n",
    "        image = image.float()\n",
    "        yp = classifier(image,text,has_image)\n",
    "        for i in range(len(text)):\n",
    "            ret.append(((text[i],image[i].cpu().detach().numpy(),label[i].cpu().detach().numpy(),has_image[i].cpu().detach().numpy()),yp[i]))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "def get_failure_cases(model_out, thresh=0.5):\n",
    "    idx = []\n",
    "    for i in range(len(model_out)):\n",
    "        (text,image,label,has_image),pred = model_out[i]\n",
    "        gt = label\n",
    "        ans = 1 if pred[1] >= thresh else 0\n",
    "        if ans != gt: idx.append(i)\n",
    "    return idx\n",
    "\n",
    "def get_metrics(model_out):\n",
    "    scores = [i[1][1] for i in model_out]\n",
    "    roc=roc_curve([i[0][2] for i in model_out], scores)\n",
    "    auc = roc_auc_score([i[0][2] for i in model_out], scores)\n",
    "    num1 = sum([i[0][2] for i in model_out])\n",
    "    num0 = len(model_out) - num1\n",
    "    fp = sum([1 if i[0][2]==0 and np.round(i[1][1])==1 else 0 for i in model_out])\n",
    "    fn = sum([1 if i[0][2]==1 and np.round(i[1][1])==0 else 0 for i in model_out])\n",
    "    tp = sum([1 if i[0][2]==1 and np.round(i[1][1])==1 else 0 for i in model_out])\n",
    "    tn = sum([1 if i[0][2]==0 and np.round(i[1][1])==0 else 0 for i in model_out])\n",
    "    correct = sum([1 if i[0][2]==np.round(i[1][1]) else 0 for i in model_out])\n",
    "    return roc,auc,fp,fn,tp,tn,num0,num1,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load(save_path(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_checkpoints/blip_deep_mlp_4/blip_deep_mlp_4_e0'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = get_val_output(classifier, val_dataloader)\n",
    "roc,auc,fp,fn,tp,tn,num0,num1,correct = get_metrics(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresh = roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9228524743230625, 23, 43, 167, 181, 204, 210, 348)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc,fp,fn,tp,tn,num0,num1,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1871018ff50>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.plot(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fails = get_failure_cases(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([model_out[fail][0][2] for fail in fails])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=14\n",
    "(text,image,label,has_image),yp = model_out[i]\n",
    "text,label,has_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "im = np.transpose(image, (1,2,0))\n",
    "\n",
    "plt.imshow(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_content_blocker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

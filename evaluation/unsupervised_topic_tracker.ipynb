{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning to find Emerging Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Use my production database to get data\n",
    "\n",
    "_POSTGRES_DB_NAME = os.environ[\"CONTENT_CURATION_POSTGRES_DB_NAME\"]\n",
    "_POSTGRES_DB_USER = os.environ[\"CONTENT_CURATION_POSTGRES_USER\"]\n",
    "_POSTGRES_DB_PASS = os.environ[\"CONTENT_CURATION_POSTGRES_PASSWORD\"]\n",
    "_POSTGRES_DB_HOST = os.environ[\"CONTENT_CURATION_POSTGRES_HOST\"]\n",
    "_POSTGRES_DB_PORT = os.environ[\"CONTENT_CURATION_POSTGRES_PORT\"]\n",
    "\n",
    "POSTGRES_DB_URL = f'postgres://{_POSTGRES_DB_USER}:{_POSTGRES_DB_PASS}@{_POSTGRES_DB_HOST}:{_POSTGRES_DB_PORT}/{_POSTGRES_DB_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing queries\n",
    "\n",
    "Make sure we can communicate with the databse and test the query with a small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(POSTGRES_DB_URL)\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    SELECT post_id,title,features,create_utc\n",
    "    FROM social_post_data NATURAL JOIN blip_features LIMIT 1;\n",
    "\"\"\")\n",
    "print(1)\n",
    "\n",
    "internal_id,title,features,create_utc = cur.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect into a CSV\n",
    "\n",
    "Take all data from August 1, 2024 to October 20, 2024 into a csv, collecting the post id, text, features, and creation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator,Tuple\n",
    "import numpy as np\n",
    "\n",
    "def post_generator(postgres_db_url, verbose=False) -> Generator[Tuple[str,np.array,int],None,None]:\n",
    "    with psycopg2.connect(postgres_db_url) as conn:\n",
    "        cur = conn.cursor()\n",
    "        # Get total count for monitoring\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT COUNT(*)\n",
    "            FROM social_post_data NATURAL JOIN blip_features\n",
    "            WHERE create_utc > %s AND create_utc < %s;\n",
    "        \"\"\", (1722470400,1729382400))\n",
    "\n",
    "        total_posts, = cur.fetchone()\n",
    "        if verbose: print(f\"Total posts: {total_posts}\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT post_id,title,features,create_utc\n",
    "            FROM social_post_data NATURAL JOIN blip_features\n",
    "            WHERE create_utc > %s AND create_utc < %s;\n",
    "        \"\"\", (1722470400,1729382400))\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                result = cur.fetchone()\n",
    "                count += 1\n",
    "                if result==None:\n",
    "                    if verbose: print(\"Super weird failure 1\")\n",
    "                    break\n",
    "                \n",
    "                post_id,title,features,create_utc = result\n",
    "                if features==None: \n",
    "                    if verbose: print(\"Super weird failure 2\")\n",
    "                    continue\n",
    "                if verbose and count%1000==0:\n",
    "                    print(f\"Processed {count}th post!\")\n",
    "                yield post_id,title,np.array([float(i) for i in features]),create_utc\n",
    "            except StopIteration:\n",
    "                if verbose: print(\"No more posts!\")\n",
    "                break\n",
    "\n",
    "        cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = post_generator(POSTGRES_DB_URL, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/blip_features_time.csv\", \"w+\") as f:\n",
    "    f.write(\"post_id\\ttitle\\tfeatures\\tcreate_utc\\n\")\n",
    "    for post_id,title,features,create_utc in generator:\n",
    "        title = title.strip().replace('\\t','')\n",
    "        f.write(f\"{post_id}\\t{title}\\t{','.join([str(i) for i in features])}\\t{create_utc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data collected above\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/blip_features_time.csv\", sep=\"\\t\")\n",
    "\n",
    "# Cleanup activity\n",
    "df = df[df[\"features\"].notnull()]\n",
    "df = df[df[\"create_utc\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "features = [np.array([float(i) for i in f.split(\",\")]) for f in list(df[\"features\"])]\n",
    "features = [f if f.shape[0]==768 else np.zeros((768,))+1e-9 for f in features]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agglom = AgglomerativeClustering(n_clusters=2, metric=\"cosine\", linkage=\"average\")\n",
    "\n",
    "clusters = agglom.fit_predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cluster\"] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot = OneHotEncoder()\n",
    "np.sum(one_hot.fit_transform(np.expand_dims(clusters, axis=1)), axis=0, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_score(features, clusters, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've tried multiple variations of clustering I'm familiar with, and expectedly, I couldn't find any meaningufl clusterings from the BLIP features. It was worth a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram emergence\n",
    "\n",
    "See if any N-gram emerges over time.\n",
    "\n",
    "Here's what I'm gonna do:\n",
    "\n",
    "Track N-gram frequencies over the whole dataset: N goes from 1 to max_N for memory conservations.\n",
    "\n",
    "Track N-gram frequencies from the last k seconds.\n",
    "\n",
    "Do a hypothesis test for each N-gram from the last k seconds, see if the frequency difference is statistically significant and greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_N = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = df[df[\"create_utc\"] <  df[\"create_utc\"].max() - 7*24*60*60]\n",
    "df_before = df_before[df_before[\"create_utc\"] >=  df[\"create_utc\"].max() - 4*7*24*60*60]\n",
    "df_after  = df[df[\"create_utc\"] >= df[\"create_utc\"].max() - 7*24*60*60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "counter = CountVectorizer(ngram_range=(1,max_N))\n",
    "\n",
    "counter.fit(list(df_after[\"title\"]))\n",
    "\n",
    "freq_after  = counter.transform([\" \".join(list(df_after[\"title\"]))])\n",
    "freq_before = counter.transform([\" \".join(list(df_before[\"title\"]))])\n",
    "\n",
    "freq_after = freq_after.toarray()\n",
    "freq_before = freq_before.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Some N-grams in df_after not in df_before. This accounts for that\n",
    "cnt_before = np.sum(\n",
    "    CountVectorizer(ngram_range=(1,max_N))\n",
    "    .fit_transform(df_before[\"title\"])\n",
    ")\n",
    "cnt_after = np.sum(freq_after)\n",
    "\n",
    "cnt_before,cnt_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_after.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor,factorial,e\n",
    "\n",
    "ngram = counter.get_feature_names_out()\n",
    "result = []\n",
    "for i in range(freq_after.shape[1]):\n",
    "    pval = 1\n",
    "    for x in range(0,floor(freq_after[0][i]*cnt_after/cnt_before+1)):\n",
    "        try:\n",
    "            pmf = (freq_before[0][i] if freq_before[0][i]>0 else 1)**x *  e**(-(freq_before[0][i] if freq_before[0][i]>0 else 1)) / factorial(x)\n",
    "            pval -= pmf\n",
    "        except OverflowError:\n",
    "            continue\n",
    "    if pval**len(ngram[i].split(\" \")) < alpha:\n",
    "        result.append((ngram[i], pval, freq_before[0][i], freq_after[0][i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-curator-curator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
